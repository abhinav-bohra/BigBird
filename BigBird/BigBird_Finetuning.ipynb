{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcZZRDx505hq"
      },
      "source": [
        "## BigBird Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://abhinav-bohra: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@github.com/abhinav-bohra/BigBird.git\n",
        "%cd BigBird\n",
        "!pip install git+https://github.com/google-research/bigbird.git -q"
      ],
      "metadata": {
        "id": "P-pjlWU0ORkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJexg2zsxfHo"
      },
      "source": [
        "## Set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0irPwcbBYvDV"
      },
      "outputs": [],
      "source": [
        "from bigbird.core import flags\n",
        "from bigbird.core import modeling\n",
        "from bigbird.core import utils\n",
        "from bigbird.summarization import run_summarization\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tft\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "if not hasattr(FLAGS, \"f\"): flags.DEFINE_string(\"f\", \"\", \"\")\n",
        "FLAGS(sys.argv)\n",
        "\n",
        "tf.enable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rph2sJ75kBNA"
      },
      "outputs": [],
      "source": [
        "FLAGS.data_dir = \"pubmed\"\n",
        "FLAGS.attention_type = \"block_sparse\"\n",
        "FLAGS.couple_encoder_decoder = True\n",
        "FLAGS.max_encoder_length = 3072  # reduce for quicker demo on free colab\n",
        "FLAGS.max_decoder_length = 256\n",
        "FLAGS.block_size = 64\n",
        "FLAGS.learning_rate = 1e-5\n",
        "FLAGS.num_train_steps = 10000\n",
        "FLAGS.attention_probs_dropout_prob = 0.0\n",
        "FLAGS.hidden_dropout_prob = 0.0\n",
        "FLAGS.use_gradient_checkpointing = True\n",
        "FLAGS.vocab_model_file = \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuxI3V_3j57Y"
      },
      "outputs": [],
      "source": [
        "transformer_config = flags.as_dictionary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRF4TUEQxjXJ"
      },
      "source": [
        "## Summarization model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RX4ow05URf8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.variable_scope import EagerVariableStore\n",
        "container = EagerVariableStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3yNdo5toQwq"
      },
      "outputs": [],
      "source": [
        "with container.as_default():\n",
        "  model = modeling.TransformerModel(transformer_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXOY78vbqHX9"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def fwd_bwd(features, labels):\n",
        "  with tf.GradientTape() as g:\n",
        "    (llh, logits, pred_ids), _ = model(features, target_ids=labels,\n",
        "                                       training=True)\n",
        "    loss = run_summarization.padded_cross_entropy_loss(\n",
        "        logits, labels,\n",
        "        transformer_config[\"label_smoothing\"],\n",
        "        transformer_config[\"vocab_size\"])\n",
        "  grads = g.gradient(loss, model.trainable_weights)\n",
        "  return loss, llh, logits, pred_ids, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzoTMyQlxsRo"
      },
      "source": [
        "## Dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo-NQTDZZs51"
      },
      "outputs": [],
      "source": [
        "train_input_fn = run_summarization.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        max_decoder_length=FLAGS.max_decoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=True)\n",
        "dataset = train_input_fn({'batch_size': 8})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D7jBCer-Kod",
        "outputId": "f9697fc8-d922-4d57-c15d-3bb72a022681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PaddedBatchDataset element_spec=(TensorSpec(shape=(8, 3072), dtype=tf.int32, name=None), TensorSpec(shape=(8, 256), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRvmfaNUi-V5"
      },
      "outputs": [],
      "source": [
        "# inspect at a few examples\n",
        "for ex in dataset.take(1):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "for example in tf.io.tf_record_iterator(\"/content/BigBird/pubmed/test.tfrecord-00000-of-00001\"):\n",
        "    print(tf.train.Example.FromString(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "a65hJnfhAH0l",
        "outputId": "3835c90e-7e33-4b09-cb68-e6e2ad08ce95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6579ef86ab2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/BigBird/pubmed/test.tfrecord-00000-of-00001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.io' has no attribute 'tf_record_iterator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYCyGH56zOOU"
      },
      "source": [
        "## Check outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uQOwyGQzRKt"
      },
      "outputs": [],
      "source": [
        "# loss, llh, logits, pred_ids, grads = fwd_bwd(ex[0], ex[1])\n",
        "# print('Loss: ', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz_LdCCdyDCR"
      },
      "source": [
        "## Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRa2dD1RzLN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daca6a0f-0a75-47ef-98e0-9f00b2a056df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 316/316 [02:25<00:00,  2.17it/s]\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = 'gs://bigbird-transformer/summarization/pubmed/roberta/model.ckpt-300000'\n",
        "ckpt_reader = tf.compat.v1.train.NewCheckpointReader(ckpt_path)\n",
        "loaded_weights = []\n",
        "for v in tqdm(model.trainable_weights, position=0):\n",
        "  try:\n",
        "    val = ckpt_reader.get_tensor(v.name[:-2])\n",
        "  except:\n",
        "    val = v.numpy()\n",
        "  loaded_weights.append(val)\n",
        "\n",
        "model.set_weights(loaded_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6-BziYxzL3U"
      },
      "source": [
        "## Train (Fine-tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWjkDvu9k7ie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "5a632274-4c32-42ff-805d-767f51dee106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fcf57e6e5fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfwd_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    820\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2921\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2924\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Feature: document (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
          ]
        }
      ],
      "source": [
        "opt = tf.keras.optimizers.Adam(FLAGS.learning_rate)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "for i, ex in enumerate(tqdm(dataset.take(FLAGS.num_train_steps), position=0)):\n",
        "  loss, llh, logits, pred_ids, grads = fwd_bwd(ex[0], ex[1])\n",
        "  opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "  train_loss(loss)\n",
        "  if i% 10 == 0:\n",
        "    print('Loss = {} '.format(train_loss.result().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXjkdtyKMAbP"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc3OatxlMCk3"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def fwd_only(features, labels):\n",
        "  (llh, logits, pred_ids), _ = model(features, target_ids=labels, training=False)\n",
        "  return llh, logits, pred_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mq_xhMzef42"
      },
      "outputs": [],
      "source": [
        "eval_input_fn = run_summarization.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        max_decoder_length=FLAGS.max_decoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=False)\n",
        "eval_dataset = eval_input_fn({'batch_size': 8})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqPN4R8kerUG"
      },
      "outputs": [],
      "source": [
        "eval_llh = tf.keras.metrics.Mean(name='eval_llh')\n",
        "\n",
        "for ex in tqdm(eval_dataset, position=0):\n",
        "  llh, logits, pred_ids = fwd_only(ex[0], ex[1])\n",
        "  eval_llh(llh)\n",
        "print('Log Likelihood = {}'.format(eval_llh.result().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-MksK2DEOn"
      },
      "source": [
        "### Print predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-FD37ZLxr8R"
      },
      "outputs": [],
      "source": [
        "tokenizer = tft.SentencepieceTokenizer(model=tf.io.gfile.GFile(FLAGS.vocab_model_file, \"rb\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RXHSgpKFG1F"
      },
      "outputs": [],
      "source": [
        "_, _, pred_ids = fwd_only(ex[0], ex[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvEFgoXJxQOa"
      },
      "outputs": [],
      "source": [
        "print('Article:\\n {}\\n\\n Predicted summary:\\n {}\\n\\n Ground truth summary:\\n {}\\n\\n'.format(\n",
        "    tokenizer.detokenize(ex[0]),\n",
        "    tokenizer.detokenize(pred_ids),\n",
        "    tokenizer.detokenize(ex[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r55AitoHmmv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data for BigBird"
      ],
      "metadata": {
        "id": "lVPFg4E1G9PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "def preprocessArticles(a_):\n",
        "    a = []\n",
        "    for line in a_:\n",
        "        if line.startswith(\"Speaker ::\") or line==\"\\n\":\n",
        "            continue\n",
        "        else:\n",
        "            a.append(line.replace('\\n',\"\").replace(\"  \",\" \"))\n",
        "    return a\n",
        "\n",
        "def preprocessSummaries(s_):\n",
        "    s = []\n",
        "    for line in s_:\n",
        "        if line.startswith(\"Speaker ::\") or line==\"\\n\":\n",
        "            continue\n",
        "        else:\n",
        "            line = line.replace('\\n',\"\").replace(\"  \",\" \")\n",
        "            line = \"<S> \" + str(line) + \" </S>\"\n",
        "            s.append(line)\n",
        "    return s\n",
        "\n",
        "\n",
        "splits = [\"train\", \"test\", \"val\"] \n",
        "data = []\n",
        "\n",
        "for split in splits:\n",
        "  path_articles = f\"/content/BigBird/data/reuters/original/{split}/ects\" \n",
        "  path_summaries = f\"/content/BigBird/data/reuters/original/{split}/gt_summaries\"\n",
        "  if split == \"val\":\n",
        "      split = \"validation\"\n",
        "  outfile = f\"/content/BigBird/data/reuters/bigbird/{split}.txt\"\n",
        "  #outfile = f\"/content/BigBird/pubmed/{split}.txt\"\n",
        "  #outfile = f\"/root/tensorflow_datasets/downloads/extracted/ZIP.ucid_1lvsqvsFi3W-pE1SqNZI0s8NR_export_download1CQHRyal4p4gv4NAVf5-_pD4o3vOCitRLkq35IcBPAQ/pubmed-dataset/{split}.txt\"\n",
        "  articles = os.listdir(path_articles)\n",
        "  summaries = os.listdir(path_summaries)\n",
        "  print(split, len(articles), len(summaries))\n",
        "  for article in articles:\n",
        "    data_point = {}\n",
        "    a_ = open(os.path.join(path_articles, article), 'r').readlines()\n",
        "    a = preprocessArticles(a_)\n",
        "    s_ = open(os.path.join(path_summaries, article), 'r').readlines()\n",
        "    s = preprocessSummaries(s_)\n",
        "    data_point[\"article_id\"] = article\n",
        "    data_point[\"article_text\"] = a\n",
        "    data_point[\"abstract_text\"] = s\n",
        "    data_point[\"section_names\"] = \"null\"\n",
        "    data_point[\"labels\"] = \"null\"\n",
        "    data_str = json.dumps(data_point)\n",
        "    data.append(data_str)\n",
        "  with open(outfile, mode='wt', encoding='utf-8') as myfile:\n",
        "    myfile.write('\\n'.join(data))\n",
        "    print(f\"{split} done\")\n",
        "  myfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7PZP5dBG_kh",
        "outputId": "0cdb1575-322b-4281-c971-bfc35d68fb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1350 1350\n",
            "train done\n",
            "test 482 482\n",
            "test done\n",
            "validation 150 150\n",
            "validation done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/pubmed\n",
        "# !tfds build"
      ],
      "metadata": {
        "id": "jiKVnGa1Ou-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /root/tensorflow_datasets/scientific_papers/pubmed/1.1.1/* /content/BigBird/pubmed/"
      ],
      "metadata": {
        "id": "C2TYjznrXIeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git config --global user.email \"abhinavbohra@iitkgp.ac.in\"\n",
        "!git config --global user.name \"abhinav-bohra\"\n",
        "!git commit -m \"Added reuters-bigbird dataset\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "TKzpFdXRO4ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "QSVSffuFSZtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nbJD8B8bSf5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1Z8Snv7SS8I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "64qy1rDPag-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BigBird - Finetuning.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}